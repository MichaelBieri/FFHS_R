Semesterarbeit Teil 4 - Michael Bieri
Aufgabe 1 a) Histogramm aller Werte
Aufgabe 1 b) Passende Verteilung mit angepassten Parametern

```{r}
# Aufgabe 1 a) Histogramm aller Werte
library(readxl)
library(ggplot2)

load("zeitdauerGesamtheit.RData")

zeitdauer <- as.numeric(zeitdauerGesamtheit$zeitdauer) # Numerisch laden der Werte
zeitdauerdf <- data.frame(zeitdauer)
zeitdauerkategoriendf <- data.frame(Zeitdauer = zeitdauer) # Datframe spaeter mit Kategorien

lambda_zeitdauer <- mean(zeitdauer) # Berechnen Mittelwerts der 'zeitdauer'
sd_zeitdauer <- sd(zeitdauer) # Berechnung Standardabweichung

print(range(zeitdauer))

# Histogramm der Zeiten
ggplot(zeitdauer_df, aes(x = zeitdauer)) +
  geom_histogram(binwidth = 1, fill = "blue", alpha = 0.7, color = "black") + # In 1er Schritten
  labs(title = "Histogramm der Zeitdauervariablen", x = "Zeitdauer", y = "Häufigkeit") +
  scale_x_continuous(breaks = seq(0, max(zeitdauer), by = 10)) + # x-Achse 10er Schritten
  scale_y_continuous(labels = scales::number_format(accuracy = 0.5)) + # y-Achse Rundung
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, size = 5))

# Kategorien
kategorien <- cut(zeitdauer, 
                  breaks = c(min(zeitdauer),0,50,100,150,200,250,300, 350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350, max(zeitdauer)), 
                  labels = c("-0", "0-50", "50-100", "100-150", "150-200", "200-250", "250-300", "300-350", "350-400", "400-450", "450-500", "500-550", "550-600", "600-650","650-700", "700-750", "750-800", "800-850", "850-900", "900-950", "950-1000", "1000-1050","1050-1100", "1100-1150", "1150-1200", "1200-1250", "1250-1300", "1300-1350", "1350+"))

zeitdauerkategoriendf$Kategorien <- kategorien # Kategorien zum Dataframe hinzufügen

# Häufigkeitsverteilung der 'zeitdauer'-Kategorien
ggplot(zeitdauerkategoriendf, aes(x = kategorien)) +
  geom_bar(color = "black", fill = "green", alpha = 0.7, na.rm = TRUE) +
  labs(title = "Histogramm der Zeitdauerkategorien", x = "Kategorien", y = "Häufigkeit") +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

# Aufgabe 1 b) Passende Verteilung mit angepassten Parametern - Normalverteilung
# Erstellen eines Dataframes für die Normalverteilung
x_values <- seq(min(zeitdauer), max(zeitdauer), length.out = 1000)
normal_data <- data.frame(
  Zeitdauer = x_values,
  Density = dnorm(x_values, mean = lambda_zeitdauer, sd = sd_zeitdauer)
)

# Normalverteilung inkl. Histogramm
ggplot() +
  geom_histogram(data = zeitdauerdf, aes(x = zeitdauer, y = ..density..), binwidth = 50, fill = "green", alpha = 0.5, color = "black") +
  geom_line(data = normal_data, aes(x = Zeitdauer, y = Density), color = "red", size = 1) +
  scale_x_continuous(breaks = seq(-50, max(zeitdauer), by = 50)) +
  labs(title = "Normalverteilung und Histogramm der Zeitreihenwerte", x = "Zeitdauer", y = "Dichtehauefigkeit pro Intervall Total = 1") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Erklaerung:
Gesamtflaeche unterhalb der Kurve oder auch Summe der Flaechen aller Bins im Histogramm gleich 1.
Dichtehöhe: Die Höhe eines Bins (z.B. 0.02) gibt die Dichte an, was bedeutet, dass in einem Intervall der Breite 1 auf der x-Achse etwa 0.02 der Gesamtflaeche des Histogramms liegt.
Flaeche eines Bins: Die Flaeche eines Bins ist die Dichtehöhe multipliziert mit der Breite des Bins. Wenn ein Bin eine Breite von 5 und eine Dichtehoehe von 0.02 hat, ist die Fläche dieses Bins 0.02 * 5 = 0.1.
Gesamtflaeche: Die Summe der Flächen aller Bins ist 1, was die gesamte Wahrscheinlichkeit darstellt. Histogramm mit Dichte: Das Histogramm zeigt die Dichte der zeitdauer-Daten.
Normalverteilungskurve: Die rote Linie zeigt die theoretische Normalverteilung basierend auf dem Mittelwert und der Standardabweichung der zeitdauer-Daten. Die Werte zwischen 75 und 125 sind also am dichtesten.

Aufgabe 1 c) Plotten Histogramm der tatsaechlichen und der theoretischen Verteilung
Die theoretische Dichteverteilung, ist oft eine kontinuierliche Dichtefunktion. Indem man die Dichtewerte der theoretischen Verteilung an den Mittelpunkten der Bins berechnet, können diese Werte mit den diskreten Höhen der Balken des Histogramms verglichen werden.
```{r}
density_values <- density(zeitdauerdf$zeitdauer)

# Tatasechliche Verteilung
ggplot(zeitdauerdf, aes(x = zeitdauer)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Observed Values", x = "Observed Value", y = "Density") +
  theme_minimal() +
  geom_line(data = data.frame(x = density_values$x, y = density_values$y), 
            aes(x = x, y = y), color = "red", size = 1)

```
```{r}
# Histogramm der tatsaechlichen Daten berechnen
hist_data <- hist(zeitdauerdf$zeitdauer, breaks = 100, plot = FALSE) # Bins

# Parameter der Normalverteilung
print(lambda_zeitdauer)
print(sd_zeitdauer)

# Hoehen der theoretischen Normalverteilung berechnen
x_values <- hist_data$mids
y_values <- dnorm(x_values, mean = lambda_zeitdauer, sd = sd_zeitdauer)
y_values <- y_values * diff(hist_data$breaks) * length(zeitdauerdf$zeitdauer) # skaliert die Dichtewerte auf die tatsächliche Anzahl der Datenpunkte und die Breite der Bins

# Balkendiagramm erstellen
barplot(height = hist_data$counts, names.arg = round(hist_data$mids, 1), col = rgb(0.2, 0.4, 0.6, 0.6), ylim = c(0, max(c(hist_data$counts, y_values))*1.2), xlab = "Zeitreihe", ylab = "Haeufigkeit", main = "Histo mit tatsaechlicher Verteilung und theoretischer Normalverteilung")
barplot(height = y_values, names.arg = round(hist_data$mids, 1), col = rgb(1, 0, 0, 0.4), add = TRUE)
legend("topright", legend = c("Tatsaechliche Verteilung", "Theoretische Normalverteilung"), fill = c(rgb(0.2, 0.4, 0.6, 0.6), rgb(1, 0, 0, 0.4)))
```
# Versuch diese nebeneinander zu plotten


```{r}
# Balkenhöhen in einer Matrix anordnen
heights <- rbind(hist_data$counts, y_values)

# Balkendiagramm erstellen mit nebeneinander stehenden Balken
barplot(heights, beside = TRUE, names.arg = round(hist_data$mids, 1),
        col = c(rgb(0.2, 0.4, 0.6, 0.6), rgb(1, 0, 0, 0.4)),
        ylim = c(0, max(heights) * 1.2),
        xlab = "Werte",
        ylab = "Haeufigkeit",
        main = "Tatsaechliche vs. Theoretische Normalverteilung",
        width = 1.5,
        space = c(0.1, 0.1))
```
# Chi-Quadrat-Anpassungstest (χ²-Goodness-of-Fit-Test)
# H0: Die Teilnehmer an den Jahren 2005 - 2014 in der Sportart Orientierungslauf von allen Teilnehmern eines Jugend und Sportkurses ist gleich verteilt. Wenn der p-Wert <0.005 deutet daraufhin, dass die beobachteten Häufigkeiten signifikant von den erwarteten Häufigkeiten abweichen, was darauf hindeutet, dass die Verteilung nicht gleichmäßig ist. Annahme gleichmaessiger Verteilung der Teilnehmenden da sonst die Erhebung der Statistik falsch waere. Jedes Jahr waren die Voraussetzungen gleich. Objektiv geht aber hervor, dass die Teilnehmenden vorallem zu Beginn stark abgenommen haben.

```{r}
library(dplyr)
library(ggplot2)

data <- read.csv("Jugend_und_Sport_Anzahl_Kurse_Teilnehmende_und_Leitende_nach_Sportart_und_Jahr_(seit 2005).csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)

# Überprüfen ob NA-Werte
sum(is.na(data))
data <- na.omit(data)

# Anzahl Personen pro Jahr bei der sportart Orientierungslauf
Teilnehmer_pro_jahr_Orientierungslauf <- data %>%
  group_by(jahr) %>%
  summarise(summe = sum(anzahl))

# Filter the data based on multiple conditions
filtered_data <- data %>%
  filter(
    sportart == "Orientierungslauf" &
    indikator == "Teilnehmende" &
    (geschlecht == "männlich" | geschlecht == "weiblich")
  )

# Group by 'jahr' and 'geschlecht' and summarize the 'anzahl'
anzahl_teilnehmende <- filtered_data %>%
  group_by(jahr) %>%
  summarize(Teilnehmer_OL_pro_Jahr = sum(anzahl, na.rm = TRUE))

str(anzahl_teilnehmende)
print(anzahl_teilnehmende)

# Darstellung der Teilnehmenden von 2005 bis 2023 an der sportart Orientierungslauf
ggplot(anzahl_teilnehmende, aes(x = anzahl_teilnehmende$jahr, y = anzahl_teilnehmende$Teilnehmer_OL_pro_Jahr)) +
  geom_line() +
  geom_point() +
  labs(title = "Teilnehmerzahlen pro Jahr", x = "Jahr", y = "Teilnehmerzahl") +
  theme_minimal()

# Erwartete Häufigkeiten annehmen, dass sie gleichmäßig verteilt sind
expected_frequencies <- rep(mean(anzahl_teilnehmende), length(anzahl_teilnehmende))

# Chi-Quadrat-Goodness-of-Fit-Test durchführen
chi_square_test <- chisq.test(anzahl_teilnehmende, p = expected_frequencies/sum(expected_frequencies))

# Testergebnis ausgeben
print(chi_square_test)
```
Der Test zeigt dass die Häufigkeiten signifikant von den erwarteten Häufigkeiten abweichen. Da p nahezu null ist.


Aufgabe 1 c)
Datn: Aktuelle Messwerte, 10 Parameter (Format csv)
Quelle: https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv
tre200s0 = Tempertatur in 2m
rre150z0 = Niederschlag in 150min
sre000z0 = Sonnenscheindauer
gre000z0 = Globalstrahlung
ure200s0 = Relative Fechtigkeit
tde200s0 = Taupunkt
```{r}
# Load the CSV file
# Load necessary library
library(dplyr)
library(psych)

# Load the CSV file
data_meteo <- read.csv("Meteodaten_VQHA80.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)
data_meteo <- data_meteo %>% select(-Station.Location)
data_meteo <- na.omit(data_meteo)

# Function to clean and convert to numeric
convert_to_numeric <- function(x) {
  x <- gsub("\\$", "", x)
  x <- gsub("-", "", x)  # Replace "-" with empty string
  as.numeric(x)
}

# Apply the function to the data
data_meteo[] <- lapply(data_meteo, convert_to_numeric)

# Print the converted data
print(data_meteo)

# Check and convert specific columns to numeric (if necessary)
data_meteo$tre200s0 <- as.numeric(data_meteo$tre200s0)
data_meteo$rre150z0 <- as.numeric(data_meteo$rre150z0)
data_meteo$sre000z0 <- as.numeric(data_meteo$sre000z0)
data_meteo$gre000z0 <- as.numeric(data_meteo$gre000z0)
data_meteo$ure200s0 <- as.numeric(data_meteo$ure200s0)
data_meteo$tde200s0 <- as.numeric(data_meteo$tde200s0)

# Calculate the correlation coefficient
correlation_coefficient_meteo <- cor(data_meteo$tre200s0, data_meteo$rre150z0, use = "complete.obs")

# Print the correlation coefficient
print(correlation_coefficient_meteo)

# Plot #1: Basic scatterplot matrix of the four measurements
pairs(~tre200s0+rre150z0+sre000z0+gre000z0+ure200s0+tde200s0, data_meteo)

# panel.smooth function
# Zeigt in der oberen rechten Ecke der Streudiagramm-Matrix die Korrelation zwischen zwei Variablen anzeigt
panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

# Plot - Fuegt den unteren Panels Loess-Glättungskurven hinzu
pairs(~tre200s0+rre150z0+sre000z0+gre000z0+ure200s0+tde200s0, data_meteo,
      lower.panel=panel.smooth, upper.panel=panel.cor, 
      pch=20, main="Streudiagramm mit Metoe-Daten")

# Final plot using the psych package
if (!require(psych)) {
  install.packages("psych")
}

pairs.panels(data_meteo[, c("tre200s0", "rre150z0", "sre000z0", "gre000z0", "ure200s0", "tde200s0")],
             method = "pearson", # correlation method
             hist.col = "#00AFBB", # histogram color
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
)

```
Diskussion:
In den unteren Plots erkennt man ob die ausgewaehlten Variablen einen Zusammenhang. In den oberen sieht man die Korrelation. Die Werte deuten auf keine Korrelation hin. Die letzte Grafik zeigt die Dichteverteilung der Datenpunkte sowie die Dichtekurven.

Aufgabe 2b)
Wir versuchen aus der unabhaengigen Variablen X die abhaengige Variable Y vorherzusagen und mit der linearen Einfachregression eine Gleichung zu finden. Der Plot residuals zeigt die Abstaende gegen dir vorhergesagten Werte. Die Punkte sind nicht um die Linie y = 0 in rot. Dies deutet nicht auf eine Homoskedastizität hin.
```{r}
# Daten in einen DataFrame packen
data <- data.frame(data_meteo)

# Lineare Regression durchführen
model <- lm(data_meteo$sre000z0 ~ data_meteo$tre200s0, data_meteo)

# Zusammenfassung des Modells anzeigen
summary(model)

# Residuen berechnen
residuals <- residuals(model)

# Residuenplot
plot(fitted(model), residuals)
abline(h = 0, col = "red")

# QQ-Plot der Residuen
qqnorm(residuals)
qqline(residuals, col = "red")
```
Scatter-Plot:
Quelle: https://opendata.swiss/de/dataset/kennzahlen-offentliche-ladeinfrastruktur-elektromobilitat
Die Datei enthaelt von den Jahren 2020 bis 2023 die Anzahl Ladestationen in verschiedenen Leistungsklassen im Total als auch in den einzelnen Kantonen. Dabei kann der Zusammenhang vom Ausbau der verschiedenen Ladestationenkategorien analysiert werden. 

Der QQ-Plot zeigt, dass die Punkte zu Beginn auf der Diagonale liegen dann aber nicht mehr. Auf der x-Achse sind die Werte 0 = Mittelwert sowie Standardabweichung = 1. Wie bereits geschrieben gibt es ab der Standardabweichungen Punkte die von der Normalverteilung abweichen.

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Load the CSV file
data_ladestationen <- read.csv("ich_tanke_strom_Kennzahlen_monatlich.csv", header = TRUE, sep = ",", stringsAsFactors = FALSE)

# Check for NA values and remove them
data_ladestationen <- na.omit(data_ladestationen)

# Filter data for the years 2020 to 2023
data_ladestationen <- data_ladestationen %>% filter(year >= 2020 & year <= 2023)

# Remove the 'month' column
data_ladestationen <- data_ladestationen %>% select(-month)

# Korrelationskoeffizient
correlation_coefficient_ladestation <- cor(data_ladestationen$chargingPower_10kW_count, data_ladestationen$chargingPower_21kW_count)

print(correlation_coefficient_ladestation)

# Create a scatter plot comparing 'chargingPower_10kW_count' and 'chargingPower_21kW_count'
ggplot(data_ladestationen, aes(x = chargingPower_10kW_count, y = chargingPower_21kW_count, color = as.factor(year))) +
  geom_point() +
  scale_color_manual(values = c("2020" = "red", "2021" = "blue", "2022" = "green", "2023" = "violet")) +
  labs(title = "Scatter Plot of Charging Power Counts",
       x = "Charging Power 10kW Count",
       y = "Charging Power 21kW Count",
       color = "Year") +
  theme_minimal()
```
Das Streudiargramm und der Korrelationskoeffizient zeigen beide eine hohe positie Korrelation.
